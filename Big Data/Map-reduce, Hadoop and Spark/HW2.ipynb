{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# HW 2- group 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import sys\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create new folders for the data and load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -mkdir /user/stud27/HW2_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -mkdir /user/stud27/HW2_data/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwxr-xr-x   - stud27 stud27          0 2021-06-16 12:02 /user/stud27/HW2_data/input\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hdfs dfs -ls /user/stud27/HW2_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -put /home/stud27/data_hw2 /user/stud27/HW2_data/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "-rw-r--r--   3 stud27 stud27        387 2021-06-16 12:02 /user/stud27/HW2_data/input/data_hw2/countries.txt\n",
      "-rw-r--r--   3 stud27 stud27    1491520 2021-06-16 12:02 /user/stud27/HW2_data/input/data_hw2/microsoft-com.data\n",
      "-rw-r--r--   3 stud27 stud27       3629 2021-06-16 12:02 /user/stud27/HW2_data/input/data_hw2/microsoft-com.info\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hdfs dfs -ls /user/stud27/HW2_data/input/data_hw2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_rdd = sc.textFile(\"hdfs:/user/stud27/HW2_data/input/data_hw2/microsoft-com.data\")\n",
    "country_rdd = sc.textFile(\"hdfs:/user/stud27/HW2_data/input/data_hw2/countries.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A,1287,1,\"International AutoRoute\",\"/autoroute\"\n",
      "A,1288,1,\"library\",\"/library\"\n",
      "A,1289,1,\"Master Chef Product Information\",\"/masterchef\"\n",
      "A,1297,1,\"Central America\",\"/centroam\"\n",
      "A,1215,1,\"For Developers Only Info\",\"/developer\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hdfs dfs -cat /user/stud27/HW2_data/input/data_hw2/microsoft-com.data | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Title: \"Anonymous web data from www.microsoft.com\"\r\n",
      "\r\n",
      "2. Source Information\r\n",
      "   -- Creators: Jack S. Breese, David Heckerman, Carl M. Kadie\r\n",
      "     -- Microsoft Research, Redmond WA, 98052-6399, USA\r\n",
      "     -- breese@microsoft.com, heckerma@microsoft.com, carlk@microsoft.com\r\n",
      "   -- Donor: Breese, Heckerman, & Kadie\r\n",
      "   -- Date: November 1998\r\n",
      " \r\n",
      "3. Past Usage:\r\n",
      "   -- J. Breese, D. Heckerman., C. Kadie _Empirical Analysis of\r\n",
      "      Predictive Algorithms for Collaborative Filtering_ Proceedings\r\n",
      "      of the Fourteenth Conference on Uncertainty in Artificial Intelligence,\r\n",
      "      Madison, WI, July, 1998. \r\n",
      "\tAlso, expanded as Microsoft Research Technical Report MSR-TR-98-12,\r\n",
      "\tThe papers are available on-line at:\r\n",
      "             http://research.microsoft.com/users/breese/cfalgs.html\r\n",
      "      -- Results: Used to compare various memory-based and model-based collobative\r\n",
      "         filtering.\r\n",
      "    - Predicted attribute: Goal was to predict what areas of the web site a\r\n",
      "         user visited based on data on what other areas he or she visited.\r\n",
      "\r\n",
      "4. Relevant Information:\r\n",
      "\r\n",
      "    We created the data by sampling and processing the www.microsoft.com logs.\r\n",
      "    The data records the use of www.microsoft.com by 30000 anonymous,\r\n",
      "    randomly-selected users. For each user, the data lists all the areas of\r\n",
      "    the web site (Vroots) that user visited in a one week timeframe.\r\n",
      "\r\n",
      "    Users are identified only by a sequential number, for example, User #14988,\r\n",
      "    User #14989, etc. The file contains no personally identifiable information.\r\n",
      "    The 294 Vroots are identified by their title (e.g. \"NetShow for PowerPoint\")\r\n",
      "    and URL (e.g. \"/stream\"). The data comes from one week in September, 1998.\r\n",
      "\r\n",
      "    Dataset format:\r\n",
      "\t-- The data is in an ASCII-based sparse-data format called \"DST\".\r\n",
      "           Each line of the data file starts with a letter which tells the line's type.\r\n",
      "           The three line types of interest are:\r\n",
      "               -- Attribute lines:\r\n",
      "\t\t     For example, 'A,1277,1,\"NetShow for PowerPoint\",\"/stream\"'\r\n",
      "                     Where:\r\n",
      "                        'A' marks this as an attribute line,\r\n",
      "                        '1277' is the attribute ID number for an area of the website\r\n",
      "                                 (called a Vroot),\r\n",
      "\t                '1' may be ignored,\r\n",
      "\t\t\t'\"NetShow for PowerPoint\"' is the title of the Vroot,\r\n",
      "                        '\"/stream\"' is the URL relative to \"http://www.microsoft.com\"\r\n",
      "                -- Vote Lines:\r\n",
      "                    For each user, there is one or more vote lines.\r\n",
      "                     For example:\r\n",
      "                           V,10164,1123,1\r\n",
      "                           V,10164,1009,1\r\n",
      "                           V,10164,1052,1\r\n",
      "                      Where:\r\n",
      "                         'V' marks the vote lines,\r\n",
      "\t\t\t\t    '10164' is the case ID number of a user,\r\n",
      "                         '1123', 1009', 1052' are the attributes ID's of Vroots that a\r\n",
      "                                user visited.\r\n",
      "                          '1' may be ignored.\r\n",
      "\r\n",
      "5. Number of Instances:\r\n",
      "      -- Training: 32711\r\n",
      "      -- Testing:   5000\r\n",
      "    Each instance represents an anonymous, randomly selected user of the web site.\r\n",
      "\r\n",
      "6. Number of Attributes: 294\r\n",
      "\r\n",
      "7. Attribute Information:\r\n",
      "   Each attribute is an area (\"vroot\") of the www.microsoft.com web site.\r\n",
      "\r\n",
      "   The datasets record which Vroots each user visited in a one-week timeframe\r\n",
      "   in September 1998.\r\n",
      "\r\n",
      "8. Missing Attribute Values: The data is very sparse, so vroot visits are explicit,\r\n",
      "    nonvisits are implicit (missing).\r\n",
      "\r\n",
      "9. Class Distribution: \r\n",
      "    Mean number of vroot visits per case: 3.0\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hdfs dfs -cat /user/stud27/HW2_data/input/data_hw2/microsoft-com.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter & map the data according to the format specified in the info files (for type 'A'- attributes)\n",
    "rdd_attributes = data_rdd.filter(lambda line: line[0] == 'A')\\\n",
    "                .map(lambda line: line.split(','))\\\n",
    "                .map(lambda line:[str(line[1]),str(line[3]).replace('\"',''),str(line[4]).replace('\"','')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1287', 'International AutoRoute', '/autoroute'], ['1288', 'library', '/library'], ['1289', 'Master Chef Product Information', '/masterchef'], ['1297', 'Central America', '/centroam'], ['1215', 'For Developers Only Info', '/developer']]\n"
     ]
    }
   ],
   "source": [
    "print rdd_attributes.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter & map the data according to the format specified in the info files (for type 'V'- votes)\n",
    "rdd_votes = data_rdd.filter(lambda line: line[0] == 'V')\\\n",
    "                .map(lambda line: line.split(','))\\\n",
    "                .map(lambda line:[str(line[2]),str(line[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1000', '10001'], ['1001', '10001'], ['1002', '10001']]\n"
     ]
    }
   ],
   "source": [
    "print rdd_votes.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Argentina', u'Australia', u'Belgium']\n"
     ]
    }
   ],
   "source": [
    "print country_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list with all the countries\n",
    "countries_lst = country_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['36585', '40557', '20911', '40720', '35548', '14196', '34279', '33731', '28995', '16424']\n"
     ]
    }
   ],
   "source": [
    "# first, filter the data that are is in the counrties list\n",
    "filter_attributes_by_countries = rdd_attributes.filter(lambda r: r[1] in countries_lst)\n",
    "# join the 2 rdd so we get only the user that visited at least 1 country from the list\n",
    "filter_users = filter_attributes_by_countries.join(rdd_votes)\n",
    "# leave only the user's id\n",
    "users = filter_users.map(lambda line : line[1][1]).distinct()\n",
    "# show the 10 first users\n",
    "print (users.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use map-reduce to count the number of user for each country.\n",
    "# from the map we get (country_name,1) and in the reduce sum the 1's for each country\n",
    "country_count_users = filter_users.map(lambda line : [line[1][0],1]).reduceByKey(lambda v1,v2: v1+v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list from the results we got above\n",
    "count_country_users_lst = country_count_users.collect()\n",
    "# get only the countries from the results we got above\n",
    "countries = country_count_users.map(lambda r : r[0]).distinct().collect()\n",
    "# for each country check if the country not in the countries with at least 1 user\n",
    "for country in countries_lst:\n",
    "    if country not in countries:\n",
    "        # add this country eith 0 in the count\n",
    "        count_country_users_lst.append((country,0)) \n",
    "# convert the list with all the countries to rdd\n",
    "count_countries = sc.parallelize(count_country_users_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>number_of_users</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jakarta</td>\n",
       "      <td>670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Germany</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Taiwan</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spain</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   country  number_of_users\n",
       "0  Jakarta              670\n",
       "1  Germany              372\n",
       "2   Sweden              258\n",
       "3   Taiwan              204\n",
       "4    Spain              191"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# order the data in DESC order by the count to get the top 5 countries\n",
    "top_5_count_countries = count_countries.takeOrdered(5, lambda x: -x[1])\n",
    "# convert to data frame to display the report\n",
    "display(pd.DataFrame(top_5_count_countries,columns=['country','number_of_users']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -put /home/stud27/data_hw2 /user/stud27/HW2_data/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert rdd to the following format: country:number_of_users and save in HDFS\n",
    "count_countries.map(lambda line:\"{}:{}\".format(line[0],line[1])).saveAsTextFile(\"hdfs:/user/stud27/HW2_data/output/country_count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average is: 3.01592736388\n"
     ]
    }
   ],
   "source": [
    "# use map reduce to get for each user the number of pages.\n",
    "# the first map reduce: from the map we got (user_id,1) and in the reduce sum the 1's for each user\n",
    "# the second map reduce: get the output from above and we got from the map ('id', (the sum of 1's,1))\n",
    "# and in the reduce we sum the total number of sums and the number of times we did it.\n",
    "# finally we caculate the average\n",
    "pages_per_user = rdd_votes.map(lambda line: (line[1],1))\\\n",
    "                          .reduceByKey(lambda v1,v2: v1+v2)\\\n",
    "                          .map(lambda line: ('id', (line[1], 1)))\\\n",
    "                          .reduceByKey(lambda v1,v2: (float(v1[0])+float(v2[0]),v1[1]+v2[1]))\\\n",
    "                          .mapValues(lambda v: v[0] / v[1]) \n",
    "\n",
    "print 'The average is:',pages_per_user.first()[1]          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The id for the page with the maximum visits is 1008 and the number of visits is 10836\n"
     ]
    }
   ],
   "source": [
    "# use map reduce to get the page id with the maximm visits\n",
    "# the first map reduce: from the map we got (page_id,1) and in the reduce sum the 1's for each page\n",
    "# the second map reduce: get the output from above and we got from the map ('id', (page_id,count_visits))\n",
    "# and in the reduce we checks each time which page id has the biggest count.\n",
    "page_max_visits = rdd_votes.map(lambda line: (line[0],1))\\\n",
    "                    .reduceByKey(lambda v1,v2: v1+v2)\\\n",
    "                    .map(lambda line: ('id',line))\\\n",
    "                    .reduceByKey(lambda v1,v2: v1 if v1[1]>v2[1] else v2)\n",
    "final_data = page_max_visits.collect()[0][1]\n",
    "print 'The id for the page with the maximum visits is',final_data[0], 'and the number of visits is', final_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove files from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/06/16 12:02:48 INFO fs.TrashPolicyDefault: Moved: 'hdfs://bdl0.eng.tau.ac.il:8020/user/stud27/HW2_data' to trash at: hdfs://bdl0.eng.tau.ac.il:8020/user/stud27/.Trash/Current/user/stud27/HW2_data\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hdfs dfs -rm -r  /user/stud27/HW2_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwx------   - stud27 stud27          0 2021-06-16 12:02 /user/stud27/.Trash\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hdfs dfs -ls  /user/stud27/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
